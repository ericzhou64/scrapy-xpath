{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persistence and Efficency\n",
    "You now might be aware of some of the problems with parsing several pages and how that can quicly get out of hand. While parsing, you might chose to persist the collected data, so that it can be analyzed and cleaned later. \n",
    "Parsing, retrieving, saving, and cleaning data are all separate actions, and you shouldn't try to work with data while collecting. In this notebook you'll practice further parsing techniques along with persistency, both for JSON and CSV formats as well as using SQL and a database.\n",
    "\n",
    "Start by loading one of the available HTML files into the `scrapy` library\n",
    "\n",
    "**Note**: This notebook relies on several dependencies that must be pre-installed in your environment. If you haven't done so, please run the following command in your terminal or command prompt:\n",
    "```bash\n",
    "python3 -m venv venv\n",
    "venv/bin/pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import os\n",
    "current_dir = os.path.abspath('')\n",
    "url = os.path.join(current_dir, \"html/1992_World_Junior_Championships_in_Athletics_â€“_Men's_high_jump\")\n",
    "with open(url, 'r', encoding='utf-8') as _f:\n",
    "    url_data = _f.read()\n",
    "\n",
    "response = scrapy.http.TextResponse(url, body=url_data, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold Steve Smith\n",
      "Silver Tim Forsyth\n",
      "Bronze Takahiro Kimino\n"
     ]
    }
   ],
   "source": [
    "# Make sure that the interesting data is available \n",
    "table = response.xpath('//table')[1].xpath('tbody')\n",
    "for tr in table.xpath('tr'):\n",
    "    print(tr.xpath('td[1]/b/text()').extract()[0],\n",
    "          tr.xpath('td[2]/a/text()').extract()[0],\n",
    "         \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This interaction with `scrapy` in a Jupyter Notebook is useful because you don't need to run the special shell and you also don't need to run the whole spider. Once you learn what you need to do here, you can adapat the spider to persist data.\n",
    "First, start by persisting data as JSON. To do this, you will need to keep the information in a Python data structure like a dictionary, and then load it as a JSON object, and finally, save it to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Gold': 'Steve Smith', 'Silver': 'Tim Forsyth', 'Bronze': 'Takahiro Kimino'}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrapped_data = {}\n",
    "for tr in table.xpath('tr'):\n",
    "    medal = tr.xpath('td/b/text()').extract()[0]\n",
    "    athlete = tr.xpath('td/a/text()').extract()[0]\n",
    "    scrapped_data[medal] = athlete\n",
    "\n",
    "scrapped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# You can convert Python into JSON first, but there is no need if you use `json.dump()`\n",
    "# as shown next\n",
    "json_data = json.dumps(scrapped_data)\n",
    "\n",
    "# Persist it in a file:\n",
    "with open(\"1992_results.json\", \"w\") as _f:\n",
    "    # use dump() with the Python dictionary directly. \n",
    "    # the conversion is done on the fly\n",
    "    json.dump(scrapped_data, _f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you can persist the scrapped data as JSON, you can also use CSV. This is specially useful if you want to to some data science operations. Although you can use an advanced library like Pandas for this, you can use the standard library CSV module from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the data first\n",
    "\n",
    "column_names = [\"Medal\", \"Athlete\"]\n",
    "rows = []\n",
    "\n",
    "for tr in table.xpath('tr'):\n",
    "    medal = tr.xpath('td/b/text()').extract()[0]\n",
    "    athlete = tr.xpath('td/a/text()').extract()[0]\n",
    "    rows.append([medal, athlete])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now persist it to disk\n",
    "import csv\n",
    "\n",
    "with open(\"1992_results.csv\", \"w\") as _f:\n",
    "    writer = csv.writer(_f)\n",
    "\n",
    "    # write the column names\n",
    "    writer.writerow(column_names)\n",
    "\n",
    "    # now write the rows\n",
    "    writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can persist data to a database. Unlike the JSON and CSV approach, using a database is much more memory efficient. This is the principal reason why you want to use a database instead of a file on disk. Imagine capturing 10GB of data. This could potentially mean that you need 10GB of available memory to hold onto that data before saving it to disk.\n",
    "By using a database, you can save the data as the data is gathered. \n",
    "\n",
    "For the next cells, use a SQLite database to persist the data. Create the file-based database and the table needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "connection = sqlite3.connect(\"1992_results.db\")\n",
    "db_table = 'CREATE TABLE IF NOT EXISTS results (id integer primary key, medal TEXT, athlete TEXT)'\n",
    "cursor = connection.cursor()\n",
    "cursor.execute(db_table)\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now it is time to persist the data. Open the connection again\n",
    "connection = sqlite3.connect(\"1992_results.db\")\n",
    "cursor = connection.cursor()\n",
    "query = 'INSERT INTO results(medal, athlete) VALUES(?, ?)'\n",
    "\n",
    "for tr in table.xpath('tr'):\n",
    "    medal = tr.xpath('td/b/text()').extract()[0]\n",
    "    athlete = tr.xpath('td/a/text()').extract()[0]\n",
    "    cursor.execute(query, (medal, athlete)) \n",
    "    connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now persisted in a file-based database that you can query. Verify that all works by creating a new connection and querying the database.\n",
    "\n",
    "Update the _wikipedia_ project and spider to use some of these techniques to persist data. Next, try parsing all the files in the _html_ directory instead of just one and persist all results. Do you think you can parse other information as well? \n",
    "\n",
    "Try parsing the height and the results from all the other athletes, not just the top three places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'Gold', 'Steve Smith')\n",
      "(2, 'Silver', 'Tim Forsyth')\n",
      "(3, 'Bronze', 'Takahiro Kimino')\n"
     ]
    }
   ],
   "source": [
    "#verify by querying the databasee\n",
    "connection = sqlite3.connect(\"1992_results.db\")\n",
    "cursor = connection.cursor()\n",
    "query = 'select * from results'\n",
    "cursor.execute(query) \n",
    "rows = cursor.fetchall()\n",
    "for row in rows:\n",
    "    print(row)\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parsing all the files in html directory \n",
    "#Create a new database called results.db and table high_jump_results\n",
    "import scrapy\n",
    "import os\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up database connection\n",
    "conn = sqlite3.connect(\"results.db\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Create table if not exists\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS high_jump_results (\n",
    "    rank TEXT,\n",
    "    athlete TEXT\n",
    ")\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all HTML files under 'html' directory and parsing througt all files\n",
    "\n",
    "html_dir = Path('html')\n",
    "html_files = list(html_dir.glob(\"*\"))  # All .html files in 'html/' folder\n",
    "\n",
    "for html_file in html_files:\n",
    "    with open(html_file, 'r', encoding='utf-8') as f:\n",
    "        url_data = f.read()\n",
    "\n",
    "    # Create Scrapy TextResponse for each file\n",
    "    response = scrapy.http.TextResponse(str(html_file), body=url_data, encoding='utf-8')\n",
    "\n",
    "   # Parse the table (adjust index [1] if the table index is different)\n",
    "    table = response.xpath('//table')[1].xpath('tbody')\n",
    "    for tr in table.xpath('tr'):\n",
    "        try:\n",
    "            rank = tr.xpath('td/b/text()').extract()[0].strip()\n",
    "            #print(rank)\n",
    "            athlete = tr.xpath('td/a/text()').extract()[0].strip()\n",
    "            #medal = tr.xpath('td/b/text()').extract()[0]\n",
    "            #athlete = tr.xpath('td/a/text()').extract()[0]\n",
    "            # Insert data into the database\n",
    "            cur.execute(\"INSERT INTO high_jump_results (rank, athlete) VALUES (?, ?)\", (rank, athlete))\n",
    "        except IndexError:\n",
    "            # Skip rows where expected data is missing\n",
    "            #print(\"no data\")\n",
    "            continue\n",
    "\n",
    "# Commit and close the connection after processing all files\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Gold', 'Steve Smith')\n",
      "('Silver', 'Tim Forsyth')\n",
      "('Bronze', 'Takahiro Kimino')\n",
      "('Gold', 'Jagan Hames')\n",
      "('Silver', 'Antoine Burke')\n",
      "('Bronze', 'Mika Polku')\n",
      "('Gold', 'Mark Boswell')\n",
      "('Silver', 'Svatoslav Ton')\n",
      "('Silver', 'Ben Challenger')\n",
      "('Gold', 'Alfredo Deza')\n",
      "('Silver', 'Yin Xueli')\n",
      "('Bronze', 'Aleksandr Veryutin')\n"
     ]
    }
   ],
   "source": [
    "#verify by querying the databasee\n",
    "conn = sqlite3.connect(\"results.db\")\n",
    "cur = conn.cursor()\n",
    "query = 'select * from high_jump_results'\n",
    "cur.execute(query) \n",
    "rows = cur.fetchall()\n",
    "for row in rows:\n",
    "    print(row)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None None\n",
      "1 Tim Forsyth 2.16\n",
      "1 Takahiro Kimino 2.16\n",
      "3 Xu Xiaodong 2.16\n",
      "3 Clifford van Reed 2.16\n",
      "5 Sven Ootjers 2.16\n",
      "6 Stanley Osuide 2.13\n",
      "6 Kristofer Lamos 2.13\n",
      "6 Antoine Burke 2.13\n",
      "6 Coenraad Roux 2.13\n",
      "10 Kim Tae-hoi 2.13\n",
      "11 Dejan MiloÅ¡evic 2.13\n",
      "12 Kostas Liapis 2.13\n",
      "13 Hugo MuÃ±oz 2.13\n",
      "14 Giorgio Florindi 2.05\n",
      "15 Oskari FrÃ¶sÃ©n 2.05\n",
      "16 Ignacio PÃ©rez 2.00\n"
     ]
    }
   ],
   "source": [
    "# paring height information from 1992 record\n",
    "import scrapy\n",
    "import os\n",
    "import sqlite3\n",
    "\n",
    "current_dir = os.path.abspath('')\n",
    "url = os.path.join(current_dir, \"html/1992_World_Junior_Championships_in_Athletics_â€“_Men's_high_jump\")\n",
    "with open(url, 'r', encoding='utf-8') as _f:\n",
    "    url_data = _f.read()\n",
    "\n",
    "response = scrapy.http.TextResponse(url, body=url_data, encoding='utf-8')\n",
    "table1 = response.xpath('//table')[3].xpath('tbody')\n",
    "for tr in table1.xpath('tr'):\n",
    "    try:\n",
    "        rank = tr.xpath('td/text()').extract_first()\n",
    "        name = tr.xpath('td/a/text()').extract_first()\n",
    "        height = tr.xpath('td/b/text()').extract_first()       \n",
    "\n",
    "        print(rank,name,height)\n",
    "\n",
    "    except AttributeError:\n",
    "        # This handles missing elements gracefully (e.g., header rows)\n",
    "        #print(\"Wrong data\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate a new table height in 1992_results.db database\n",
    "import sqlite3\n",
    "import scrapy\n",
    "import os\n",
    "\n",
    "connection = sqlite3.connect(\"1992_jump_results.db\")\n",
    "db_table = 'CREATE TABLE IF NOT EXISTS heights (id integer primary key, rank TEXT, athlete TEXT, height TEXT)'\n",
    "cursor = connection.cursor()\n",
    "cursor.execute(db_table)\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None None\n",
      "1 Tim Forsyth 2.16\n",
      "1 Takahiro Kimino 2.16\n",
      "3 Xu Xiaodong 2.16\n",
      "3 Clifford van Reed 2.16\n",
      "5 Sven Ootjers 2.16\n",
      "6 Stanley Osuide 2.13\n",
      "6 Kristofer Lamos 2.13\n",
      "6 Antoine Burke 2.13\n",
      "6 Coenraad Roux 2.13\n",
      "10 Kim Tae-hoi 2.13\n",
      "11 Dejan MiloÅ¡evic 2.13\n",
      "12 Kostas Liapis 2.13\n",
      "13 Hugo MuÃ±oz 2.13\n",
      "14 Giorgio Florindi 2.05\n",
      "15 Oskari FrÃ¶sÃ©n 2.05\n",
      "16 Ignacio PÃ©rez 2.00\n"
     ]
    }
   ],
   "source": [
    "current_dir = os.path.abspath('')\n",
    "url = os.path.join(current_dir, \"html/1992_World_Junior_Championships_in_Athletics_â€“_Men's_high_jump\")\n",
    "with open(url, 'r', encoding='utf-8') as _f:\n",
    "    url_data = _f.read()\n",
    "\n",
    "response = scrapy.http.TextResponse(url, body=url_data, encoding='utf-8')\n",
    "table1 = response.xpath('//table')[3].xpath('tbody')\n",
    "for tr in table1.xpath('tr'):\n",
    "    try:\n",
    "        rank = tr.xpath('td/text()').extract_first()\n",
    "        name = tr.xpath('td/a/text()').extract_first()\n",
    "        height = tr.xpath('td/b/text()').extract_first()       \n",
    "        print(rank,name,height)\n",
    "        cursor.execute(\"INSERT INTO heights (rank, athlete, height) VALUES (?, ?, ?)\", (rank, name,height))\n",
    "        connection.commit()\n",
    "    except AttributeError:\n",
    "        # This handles missing elements gracefully (e.g., header rows)\n",
    "        print(\"Wrong data\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, None, None, None)\n",
      "(2, '1', 'Tim Forsyth', '2.16')\n",
      "(3, '1', 'Takahiro Kimino', '2.16')\n",
      "(4, '3', 'Xu Xiaodong', '2.16')\n",
      "(5, '3', 'Clifford van Reed', '2.16')\n",
      "(6, '5', 'Sven Ootjers', '2.16')\n",
      "(7, '6', 'Stanley Osuide', '2.13')\n",
      "(8, '6', 'Kristofer Lamos', '2.13')\n",
      "(9, '6', 'Antoine Burke', '2.13')\n",
      "(10, '6', 'Coenraad Roux', '2.13')\n",
      "(11, '10', 'Kim Tae-hoi', '2.13')\n",
      "(12, '11', 'Dejan MiloÅ¡evic', '2.13')\n",
      "(13, '12', 'Kostas Liapis', '2.13')\n",
      "(14, '13', 'Hugo MuÃ±oz', '2.13')\n",
      "(15, '14', 'Giorgio Florindi', '2.05')\n",
      "(16, '15', 'Oskari FrÃ¶sÃ©n', '2.05')\n",
      "(17, '16', 'Ignacio PÃ©rez', '2.00')\n"
     ]
    }
   ],
   "source": [
    "#verify by querying the databasee\n",
    "connection = sqlite3.connect(\"1992_jump_results.db\")\n",
    "cur = connection.cursor()\n",
    "query = 'select * from heights'\n",
    "cur.execute(query) \n",
    "rows = cur.fetchall()\n",
    "for row in rows:\n",
    "    print(row)\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0303190f1c8c691fa9994d3c799a212d90e80d675371cad4184da4200e89748e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
